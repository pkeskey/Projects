{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# To run code have to run this first\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tPd0U--kNBD","executionInfo":{"status":"ok","timestamp":1747069776806,"user_tz":300,"elapsed":566,"user":{"displayName":"Preston Keskey","userId":"02221095565337972702"}},"outputId":"470f8c30-7a6c-487f-c43c-5b4f8d975a2b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# remove the incompatible copy you just installed\n","!pip uninstall -y numpy\n","\n","# install the last 1.26 wheel (ABI‑compatible with all current libs)\n","!pip install --quiet \"numpy==1.26.4\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dvcv-vQqmlqr","executionInfo":{"status":"ok","timestamp":1747069785512,"user_tz":300,"elapsed":6286,"user":{"displayName":"Preston Keskey","userId":"02221095565337972702"}},"outputId":"02d12ecc-9762-4b2b-aed0-fd54ace93b6f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: numpy 1.26.4\n","Uninstalling numpy-1.26.4:\n","  Successfully uninstalled numpy-1.26.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install --quiet --force-reinstall --no-cache-dir \\\n","  scipy==1.12.0 pandas==2.2.2 scikit-learn==1.5.0 statsmodels==0.14.1\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y5c9T9m9mpSG","executionInfo":{"status":"ok","timestamp":1747069827234,"user_tz":300,"elapsed":24627,"user":{"displayName":"Preston Keskey","userId":"02221095565337972702"}},"outputId":"4de1e4ad-ddea-49c3-cd48-55802538b379"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m282.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m175.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m264.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m212.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m362.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m297.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m218.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.9/232.9 kB\u001b[0m \u001b[31m351.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m343.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m360.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m365.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","langchain-core 0.3.59 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["# 1.  Core PyTorch stack (keeps numpy==1.26.x)\n","!pip install --quiet --upgrade \\\n","  torch==2.2.2+cu121 torchvision==0.17.2+cu121 torchaudio==2.2.2+cu121 \\\n","  --index-url https://download.pytorch.org/whl/cu121\n","\n","# 2.  Everything else (do NOT touch numpy)\n","!pip install --quiet --upgrade \\\n","  statsmodels==0.14.1 pmdarima==2.0.4 properscoring==0.1 \\\n","  optuna==3.6.1 tqdm==4.67.1 scikit-learn==1.5.0\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xQXoFYa8msAo","executionInfo":{"status":"ok","timestamp":1747069655178,"user_tz":300,"elapsed":135485,"user":{"displayName":"Preston Keskey","userId":"02221095565337972702"}},"outputId":"386de3ba-51b8-48dc-c387-ce60e9b64809"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["# BARNN Model"],"metadata":{"id":"RTvILkThjHWJ"}},{"cell_type":"code","source":["# ╭──────────────────────  BARNN‑medium‑fast.py  ───────────────────────╮\n","# Bayesian Autoregressive RNN  (dispersion + K=5 VAMP mixture)\n","# Identical model & training;   evaluation is now O(N) instead of O(N·34)\n","# Prints:  RMSLE / RMSE / MAE / 90 %‑coverage / CRPS\n","# Runs ~2‑3 min total on a Colab A100 instance\n","# ╰──────────────────────────────────────────────────────────────────────╯\n","import os, random, warnings, math, numpy as np, pandas as pd\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.distributions import NegativeBinomial\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from properscoring import crps_ensemble\n","\n","# ─── hyper‑params ───────────────────────────────────────────────────────\n","SEED, N_EPOCHS, BATCH_SIZE, LR = 42, 4, 2_048, 3e-4\n","SEQ_LEN, K_VAMP, N_MC          = 12, 5, 10\n","TRAIN_CUTOFF                   = 31          # months 0‑30 train, 31 eval\n","DATA_DIR                       = \"/content/drive/MyDrive/CSCI 5527 Project\"\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","# ────────────────────────────────────────────────────────────────────────\n","warnings.filterwarnings(\"ignore\")\n","torch.backends.cudnn.benchmark = True\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","\n","# 1 ─── DATA (exactly as before) ─────────────────────────────────────────\n","print(\"[Data] loading …\")\n","sales = pd.read_csv(f\"{DATA_DIR}/sales_train.csv\")\n","monthly = (sales.groupby([\"date_block_num\",\"shop_id\",\"item_id\"], as_index=False)\n","                 .agg(item_cnt_month=(\"item_cnt_day\", \"sum\")))\n","\n","grid = (monthly[[\"shop_id\",\"item_id\"]].drop_duplicates().assign(key=1)\n","          .merge(pd.DataFrame({\"date_block_num\": range(34), \"key\": 1}), on=\"key\")\n","          .drop(\"key\", axis=1))\n","\n","monthly = (grid.merge(monthly, how=\"left\")\n","                .fillna({\"item_cnt_month\": 0})\n","                .sort_values([\"shop_id\",\"item_id\",\"date_block_num\"]))\n","\n","shop2i = {sid: i for i, sid in enumerate(sorted(monthly.shop_id.unique()))}\n","item2i = {iid: i for i, iid in enumerate(sorted(monthly.item_id.unique()))}\n","monthly[\"shop_idx\"] = monthly.shop_id.map(shop2i)\n","monthly[\"item_idx\"] = monthly.item_id.map(item2i)\n","\n","N_SHOPS, N_ITEMS = len(shop2i), len(item2i)\n","\n","# 2 ─── metric helpers ───────────────────────────────────────────────────\n","_clip = lambda x: np.maximum(0, np.asarray(x))\n","def rmsle(y_t, y_p): return np.sqrt(np.mean((np.log1p(_clip(y_p)) -\n","                                             np.log1p(_clip(y_t)))**2))\n","def rmse (y_t, y_p): return np.sqrt(mean_squared_error(y_t, _clip(y_p)))\n","def mae  (y_t, y_p): return mean_absolute_error(y_t, _clip(y_p))\n","def coverage(y, lo, hi): return ((_clip(y) >= lo) & (_clip(y) <= hi)).mean()\n","\n","# 3 ─── Dataset (unchanged) ──────────────────────────────────────────────\n","class SalesSeq(Dataset):\n","    def __init__(self, frame, seq_len, mode):\n","        self.seq_len, self.mode = seq_len, mode\n","        self.series = {k: g.sort_values(\"date_block_num\")\n","                           .item_cnt_month.values\n","                       for k, g in frame.groupby([\"shop_idx\",\"item_idx\"])}\n","        self.keys = list(self.series)\n","\n","    def __len__(self): return len(self.keys)\n","\n","    def __getitem__(self, idx):\n","        s, i = self.keys[idx]; vec = self.series[(s, i)]\n","        if self.mode == \"train\":\n","            t = np.random.randint(self.seq_len, TRAIN_CUTOFF)\n","            x, y = vec[t-self.seq_len:t], vec[t]\n","        else:\n","            x = vec[TRAIN_CUTOFF-self.seq_len:TRAIN_CUTOFF]\n","            y = vec[TRAIN_CUTOFF]\n","        return (torch.tensor(s), torch.tensor(i),\n","                torch.tensor(x, dtype=torch.float32),\n","                torch.tensor(y, dtype=torch.float32))\n","\n","train_ds = SalesSeq(monthly, SEQ_LEN, \"train\")\n","eval_ds  = SalesSeq(monthly, SEQ_LEN, \"eval\")\n","loader   = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n","                      num_workers=2, pin_memory=True)\n","\n","# 4 ─── BARNN model definition  (identical to the last version) ───────────\n","class DropoutEncoder(nn.Module):\n","    def __init__(self, n_layers, k, hid=128):\n","        super().__init__(); self.k = k\n","        self.fc = nn.Sequential(nn.Linear(SEQ_LEN+1, hid), nn.LeakyReLU(),\n","                                nn.Linear(hid, hid),        nn.LeakyReLU())\n","        self.head_alpha = nn.Linear(hid, n_layers*k)\n","        self.head_pi    = nn.Linear(hid, k)\n","    def forward(self, hist, step):\n","        z = self.fc(torch.cat([hist, step.unsqueeze(1)/100], 1))\n","        alpha = torch.exp(self.head_alpha(z)).clamp_max(3.).view(hist.size(0), 2, self.k)\n","        pi    = torch.softmax(self.head_pi(z), 1)\n","        return alpha, pi\n","\n","class VariationalGRU(nn.Module):\n","    def __init__(self, inp, hid):\n","        super().__init__(); self.gru = nn.GRUCell(inp, hid); self.out = nn.Linear(hid,1)\n","    @staticmethod\n","    def _mask(w, a): return (a * w) * (1 + torch.randn_like(w))\n","    def forward(self, x, h, a):\n","        a_r, a_o = a[:,0].mean().clamp_max(3), a[:,1].mean().clamp_max(3)\n","        with torch.no_grad():\n","            self.gru.weight_ih.copy_(self._mask(self.gru.weight_ih, a_r))\n","            self.out.weight.copy_(self._mask(self.out.weight, a_o))\n","        h = self.gru(x, h); return self.out(h), h\n","\n","class BARNN(nn.Module):\n","    def __init__(self, n_shops, n_items, emb=16, hid=64, k=5):\n","        super().__init__(); self.k = k\n","        self.shop_emb = nn.Embedding(n_shops, emb)\n","        self.item_emb = nn.Embedding(n_items, emb)\n","        self.enc  = DropoutEncoder(2, k); self.rnn = VariationalGRU(1+2*emb, hid)\n","        self.log_theta = nn.Parameter(torch.zeros(1))\n","    def forward(self, s, i, hist, step):\n","        α_k, π = self.enc(hist, step)\n","        g = -torch.log(-torch.log(torch.rand_like(π)+1e-8)+1e-8)\n","        k_idx = torch.argmax((torch.log(π+1e-9)+g) / 0.5, 1)\n","        α = α_k[torch.arange(hist.size(0)), :, k_idx]\n","        h0 = torch.zeros(hist.size(0), self.rnn.gru.hidden_size, device=hist.device)\n","        inp = torch.cat([hist[:, -1:], self.shop_emb(s), self.item_emb(i)], 1)\n","        log_μ, _ = self.rnn(inp, h0, α); log_μ = log_μ.clamp(-10, 10)\n","        μ = torch.exp(log_μ).squeeze(1)\n","        θ = torch.exp(self.log_theta.clamp(-4, 4))\n","        r = θ.expand_as(μ); p = (θ / (θ + μ)).clamp(1e-6, 1-1e-6)\n","        return r, p, α, π\n","\n","def kl_dropout(α):\n","    β = α.mean(0); γ = torch.sqrt((α**2).mean(0))\n","    return (0.5*((α-β)/γ)**2 + 0.5*(α/γ)**2 - 0.5 -\n","            torch.log(α/γ + 1e-8)).mean()\n","def kl_mixture(π, k): return (π * (π.clamp(1e-8).log() + math.log(k))).sum(1).mean()\n","\n","# 5 ─── Training (unchanged) ──────────────────────────────────────────────\n","model = BARNN(N_SHOPS, N_ITEMS, k=K_VAMP).to(DEVICE)\n","opt   = torch.optim.AdamW(model.parameters(), lr=LR)\n","\n","print(\"\\n[BARNN] training …\")\n","for ep in range(1, N_EPOCHS+1):\n","    model.train(); ep_loss = 0; n = 0\n","    for s, i, x, y in loader:\n","        s, i, x = s.to(DEVICE), i.to(DEVICE), x.to(DEVICE)\n","        y_i     = y.round().clamp_min(0).long().to(DEVICE)\n","        step    = torch.full_like(y, ep, dtype=torch.float32).to(DEVICE)\n","        r, p, α, π = model(s, i, x, step)\n","        nll  = -NegativeBinomial(r, p).log_prob(y_i).mean()\n","        loss = nll + 1e-3*kl_dropout(α) + 1e-3*kl_mixture(π, K_VAMP)\n","        opt.zero_grad(); loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        opt.step()\n","        ep_loss += loss.item()*y.size(0); n += y.size(0)\n","    print(f\"[{ep}/{N_EPOCHS}] loss {ep_loss/n:.4f}\")\n","\n","# 6 ─── Fast evaluation  (no slow DataFrame loop) ─────────────────────────\n","print(\"\\n[BARNN] MC inference …\")\n","model.eval()\n","\n","# --- build the (N_series × 12) history tensor directly from cached vectors\n","hist_stack = np.stack([v[TRAIN_CUTOFF-SEQ_LEN:TRAIN_CUTOFF]\n","                       for v in eval_ds.series.values()])     # ±1 s\n","\n","S = torch.arange(len(eval_ds), dtype=torch.long, device=DEVICE) // N_ITEMS\n","I = torch.arange(len(eval_ds), dtype=torch.long, device=DEVICE) %  N_ITEMS\n","H = torch.tensor(hist_stack, dtype=torch.float32, device=DEVICE)\n","step0 = torch.zeros(len(eval_ds), dtype=torch.float32, device=DEVICE)\n","\n","samples = []\n","with torch.no_grad():\n","    for _ in range(N_MC):\n","        r, p, _, _ = model(S, I, H, step0)\n","        samples.append((p * r / (1 - p)).cpu().numpy())\n","samples = np.stack(samples)                     # (MC, N_series)\n","\n","mu = _clip(samples.mean(0))\n","lo = _clip(np.percentile(samples, 5, 0))\n","hi = _clip(np.percentile(samples, 95, 0))\n","true = monthly.loc[monthly.date_block_num == TRAIN_CUTOFF,\n","                   \"item_cnt_month\"].values\n","\n","print(f\"BARNN | RMSLE {rmsle(true, mu):.4f} | RMSE {rmse(true, mu):.2f}\"\n","      f\" | MAE {mae(true, mu):.2f} | Cov90 {coverage(true, lo, hi):.3f}\"\n","      f\" | CRPS {crps_ensemble(true, np.stack([mu]*N_MC, 1)).mean():.3f}\")\n","# ╰───────────────────────────────────────────────────────────────────────╯"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25FwZ6Gjipe1","executionInfo":{"status":"ok","timestamp":1746733754513,"user_tz":300,"elapsed":279843,"user":{"displayName":"Preston Keskey","userId":"02221095565337972702"}},"outputId":"2b5b6f8f-d177-44d5-d577-71fb056eed2b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[Data] loading …\n","\n","[BARNN] training …\n","[1/4] loss 2.1055\n","[2/4] loss 0.8383\n","[3/4] loss 1.4443\n","[4/4] loss 0.8661\n","\n","[BARNN] MC inference …\n","BARNN | RMSLE 0.5192 | RMSE 1.68 | MAE 0.71 | Cov90 0.028 | CRPS 0.709\n"]}]},{"cell_type":"code","source":["# ╭────────────────────────  barnn_paper_repro.py  ───────────────────────╮\n","# Minimal, faithful implementation of Coscia et al. (2025) – *fixed row_mask*\n","# ╰───────────────────────────────────────────────────────────────────────╯\n","import os, math, random, warnings, numpy as np, pandas as pd, torch\n","import torch.nn as nn, torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.distributions import NegativeBinomial\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from properscoring import crps_ensemble\n","\n","# ─── configuration ──────────────────────────────────────────────────────\n","SEED            = 42\n","EPOCHS, WARMUP  = 15, 5\n","BATCH           = 2_048\n","BASE_LR         = 3e-4\n","SEQ_LEN, K_MIX  = 12, 10\n","HID,  EMB       = 128, 32\n","MC_PASSES       = 20\n","TRAIN_CUT       = 31\n","DEVICE          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","DATA_DIR        = \"/content/drive/MyDrive/CSCI 5527 Project\"\n","warnings.filterwarnings(\"ignore\"); torch.backends.cudnn.benchmark = True\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","\n","# ─── 1. data – identical to baseline scripts ────────────────────────────\n","print(\"[Data] loading …\")\n","sales = pd.read_csv(f\"{DATA_DIR}/sales_train.csv\")\n","monthly = (sales.groupby([\"date_block_num\",\"shop_id\",\"item_id\"],as_index=False)\n","                 .agg(item_cnt_month=(\"item_cnt_day\",\"sum\")))\n","grid = (monthly[[\"shop_id\",\"item_id\"]].drop_duplicates().assign(key=1)\n","          .merge(pd.DataFrame({\"date_block_num\":range(34),\"key\":1}),on=\"key\")\n","          .drop(\"key\",axis=1))\n","monthly = (grid.merge(monthly,how=\"left\").fillna({\"item_cnt_month\":0})\n","                   .sort_values([\"shop_id\",\"item_id\",\"date_block_num\"]))\n","shop2i = {s:i for i,s in enumerate(sorted(monthly.shop_id.unique()))}\n","item2i = {t:i for i,t in enumerate(sorted(monthly.item_id.unique()))}\n","monthly[\"shop_idx\"] = monthly.shop_id.map(shop2i)\n","monthly[\"item_idx\"] = monthly.item_id.map(item2i)\n","N_SHOPS, N_ITEMS = len(shop2i), len(item2i)\n","\n","_clip = lambda x: np.maximum(0,np.asarray(x))\n","def rmsle(t,p): return np.sqrt(np.mean((np.log1p(_clip(p))-np.log1p(_clip(t)))**2))\n","def rmse (t,p): return np.sqrt(mean_squared_error(t,_clip(p)))\n","def mae  (t,p): return mean_absolute_error(t,_clip(p))\n","def cov90(t,lo,hi): return ((_clip(t)>=lo)&(_clip(t)<=hi)).mean()\n","\n","# ─── 2. PyTorch dataset ─────────────────────────────────────────────────\n","class Sales(Dataset):\n","    def __init__(self,frame,L,mode):\n","        self.L,self.mode=L,mode\n","        self.series={k:g.sort_values(\"date_block_num\").item_cnt_month.values\n","                     for k,g in frame.groupby([\"shop_idx\",\"item_idx\"])}\n","        self.keys=list(sorted(self.series))\n","    def __len__(self): return len(self.keys)\n","    def __getitem__(self,idx):\n","        s,i=self.keys[idx]; vec=self.series[(s,i)]\n","        if self.mode==\"train\":\n","            t=np.random.randint(self.L,TRAIN_CUT); x,y=vec[t-self.L:t],vec[t]\n","        else:\n","            x=vec[TRAIN_CUT-self.L:TRAIN_CUT]; y=vec[TRAIN_CUT]\n","        return (torch.tensor(s),torch.tensor(i),\n","                torch.tensor(x,dtype=torch.float32),\n","                torch.tensor(y,dtype=torch.float32))\n","tr_ds, ev_ds = Sales(monthly,SEQ_LEN,\"train\"), Sales(monthly,SEQ_LEN,\"eval\")\n","loader = DataLoader(tr_ds,batch_size=BATCH,shuffle=True,num_workers=2,pin_memory=True)\n","\n","# ─── 3. model components ────────────────────────────────────────────────\n","def row_mask(w, alpha_scalar):\n","    \"\"\"Apply the *same scalar* dropout‑noise to every row (eq. 7,8).\"\"\"\n","    return w * (1 + torch.randn_like(w) * alpha_scalar)     # <— fixed\n","\n","class NoisyGRUCell(nn.Module):\n","    def __init__(self,inp,hid):\n","        super().__init__(); self.hid=hid\n","        self.W_ih=nn.Parameter(torch.empty(3*hid,inp))\n","        self.W_hh=nn.Parameter(torch.empty(3*hid,hid))\n","        self.b_ih=nn.Parameter(torch.zeros(3*hid))\n","        self.b_hh=nn.Parameter(torch.zeros(3*hid))\n","        self.out = nn.Linear(hid,1)\n","        nn.init.xavier_uniform_(self.W_ih); nn.init.orthogonal_(self.W_hh)\n","    def forward(self,x,h_prev,alpha_s):\n","        Wi = row_mask(self.W_ih, alpha_s)\n","        Wo = row_mask(self.out.weight, alpha_s)\n","        gi = F.linear(x,      Wi, self.b_ih)\n","        gh = F.linear(h_prev, self.W_hh, self.b_hh)\n","        i_r,i_i,i_n = gi.chunk(3,1); h_r,h_i,h_n = gh.chunk(3,1)\n","        reset = torch.sigmoid(i_r+h_r); upd = torch.sigmoid(i_i+h_i)\n","        new  = torch.tanh(i_n + reset*h_n)\n","        h    = new + upd*(h_prev-new)\n","        y    = F.linear(h, Wo, self.out.bias)\n","        return y,h\n","\n","class Encoder(nn.Module):\n","    def __init__(self,L,k,h=256):\n","        super().__init__(); self.k=k\n","        self.net=nn.Sequential(nn.Linear(L+1,h),nn.GELU(),\n","                               nn.Linear(h,h),nn.GELU())\n","        self.ha = nn.Linear(h,2*k); self.hpi=nn.Linear(h,k)\n","        self.register_buffer(\"beta\", torch.ones(2))\n","        self.register_buffer(\"gamma\",torch.ones(2))\n","        self.decay=0.99\n","    def forward(self,hist,step):\n","        z=self.net(torch.cat([hist,step.unsqueeze(1)/100],1))\n","        a_raw=torch.exp(self.ha(z)).view(hist.size(0),2,self.k)\n","        with torch.no_grad():\n","            self.beta  = self.decay*self.beta  + (1-self.decay)*a_raw.mean(0).mean(-1)\n","            self.gamma = self.decay*self.gamma + (1-self.decay)*a_raw.pow(2).mean(0).mean(-1).sqrt()\n","        return a_raw.clamp_max(6.), torch.softmax(self.hpi(z),1)\n","\n","class BARNN(nn.Module):\n","    def __init__(self,nS,nI):\n","        super().__init__()\n","        self.se, self.ie = nn.Embedding(nS,EMB), nn.Embedding(nI,EMB)\n","        self.enc  = Encoder(SEQ_LEN,K_MIX)\n","        self.g1   = NoisyGRUCell(1+2*EMB,HID)\n","        self.g2   = NoisyGRUCell(HID,HID)\n","        self.thS, self.thI = nn.Embedding(nS,1), nn.Embedding(nI,1)\n","    def forward(self,s,i,hist,step):\n","        αK,π = self.enc(hist,step)\n","        g=-torch.log(-torch.log(torch.rand_like(π)+1e-8)+1e-8)\n","        k=torch.argmax((torch.log(π+1e-9)+g)/0.5,1)\n","        α = αK[torch.arange(hist.size(0),device=hist.device),:,k]   # (B,2)\n","        α_s = 0.25*torch.tanh(α.mean())                             # scalar\n","        h0=torch.zeros(hist.size(0),HID,device=hist.device)\n","        inp=torch.cat([hist[:,-1:],self.se(s),self.ie(i)],1)\n","        y1,h1=self.g1(inp,h0,α_s); y2,_=self.g2(h1,h1,α_s)\n","        mu=torch.exp((y1+y2).clamp(-8,8).squeeze(1))\n","        θ = F.softplus(self.thS(s)+self.thI(i)).squeeze(1)+1e-3\n","        p = torch.clamp(θ/(θ+mu),1e-5,1-1e-5)\n","        return θ, p, α\n","\n","def kl_alpha(α,b,γ):\n","    return (0.5*((α-b)/γ)**2 + 0.5*(α/γ)**2 -0.5 - torch.log(α/γ+1e-8)).mean()\n","\n","# ─── 4. training loop ───────────────────────────────────────────────────\n","model=BARNN(N_SHOPS,N_ITEMS).to(DEVICE)\n","opt  = torch.optim.AdamW(model.parameters(),lr=BASE_LR,weight_decay=1e-4)\n","tot=len(loader)*EPOCHS\n","sched=torch.optim.lr_scheduler.LambdaLR(\n","    opt, lambda s:(s+1)/WARMUP if s<WARMUP else 0.5*(1+math.cos(\n","        math.pi*(s-WARMUP)/(tot-WARMUP))))\n","\n","print(\"\\n[BARNN] training …\")\n","for ep in range(1,EPOCHS+1):\n","    model.train(); tot_loss=n=0\n","    for sh,it,x,y in loader:\n","        sh,it,x=sh.to(DEVICE),it.to(DEVICE),x.to(DEVICE)\n","        y_i=y.round().clamp_min(0).long().to(DEVICE)\n","        st=torch.full_like(y,ep,dtype=torch.float32,device=DEVICE)\n","        r,p,α=model(sh,it,x,st)\n","        loss=-NegativeBinomial(r,p).log_prob(y_i).mean() \\\n","              +1e-3*kl_alpha(α,model.enc.beta,model.enc.gamma)\n","        opt.zero_grad(); loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n","        opt.step(); sched.step()\n","        tot_loss+=loss.item()*y.size(0); n+=y.size(0)\n","    print(f\"[{ep:2}/{EPOCHS}] loss {tot_loss/n:.4f}\")\n","\n","# ... everything up to model.eval() is identical ...\n","\n","# ─── 5. MC evaluation (fixed CRPS shapes) ───────────────────────────────\n","print(\"\\n[BARNN] MC inference …\")\n","model.eval()\n","order = sorted(ev_ds.series.items())\n","hist  = np.stack([v[TRAIN_CUT-SEQ_LEN:TRAIN_CUT] for _, v in order])\n","Sids  = torch.tensor([k[0] for k,_ in order], dtype=torch.long, device=DEVICE)\n","Iids  = torch.tensor([k[1] for k,_ in order], dtype=torch.long, device=DEVICE)\n","Hist  = torch.tensor(hist, dtype=torch.float32, device=DEVICE)\n","step0 = torch.zeros(len(ev_ds), dtype=torch.float32, device=DEVICE)\n","\n","ensemble = []\n","with torch.no_grad():\n","    for _ in range(MC_PASSES):\n","        r, p, _ = model(Sids, Iids, Hist, step0)\n","        ensemble.append(NegativeBinomial(r, p).sample().cpu().numpy())\n","\n","ensemble = np.stack(ensemble).T          # (N_series , MC)\n","\n","mu = _clip(ensemble.mean(-1))\n","lo = _clip(np.percentile(ensemble, 5 , -1))\n","hi = _clip(np.percentile(ensemble, 95, -1))\n","\n","true = monthly.loc[monthly.date_block_num == TRAIN_CUT,\n","                   \"item_cnt_month\"].values\n","\n","print(f\"BARNN | RMSLE {rmsle(true, mu):.4f} | RMSE {rmse(true, mu):.2f} \"\n","      f\"| MAE {mae(true, mu):.2f} | Cov90 {cov90(true, lo, hi):.3f} \"\n","      f\"| CRPS {crps_ensemble(true, ensemble).mean():.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AGnBbdKF1clz","executionInfo":{"status":"ok","timestamp":1746741858914,"user_tz":300,"elapsed":408653,"user":{"displayName":"Preston Keskey","userId":"02221095565337972702"}},"outputId":"6ea51678-0b15-411c-b272-c8e0e6fc4d1e"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[Data] loading …\n","\n","[BARNN] training …\n","[ 1/15] loss 0.7051\n","[ 2/15] loss 0.5706\n","[ 3/15] loss 0.5537\n","[ 4/15] loss 0.5406\n","[ 5/15] loss 0.5359\n","[ 6/15] loss 0.5366\n","[ 7/15] loss 0.5292\n","[ 8/15] loss 0.5142\n","[ 9/15] loss 0.5091\n","[10/15] loss 0.5112\n","[11/15] loss 0.5137\n","[12/15] loss 0.5062\n","[13/15] loss 0.5023\n","[14/15] loss 0.5026\n","[15/15] loss 0.4967\n","\n","[BARNN] MC inference …\n","BARNN | RMSLE 0.3225 | RMSE 2.28 | MAE 0.31 | Cov90 0.960 | CRPS 0.173\n"]}]}]}