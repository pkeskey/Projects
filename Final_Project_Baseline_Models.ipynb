{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMyjJj2v2yxVnvs78TBAFQ7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ltrx5FQI8wnf","executionInfo":{"status":"ok","timestamp":1746645116099,"user_tz":300,"elapsed":517,"user":{"displayName":"Preston Keskey","userId":"02221095565337972702"}},"outputId":"84b26870-46fa-4a9f-fd7c-f0b3726c9256"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# To run code have to run this first\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# ╭────────────── Cell 1 : Core PyTorch stack (cu121) ─────────────╮\n","#  ▸ installs matching torch/torchvision/torchaudio wheels\n","!pip install --quiet --upgrade \\\n","  torch==2.2.2+cu121 torchvision==0.17.2+cu121 torchaudio==2.2.2+cu121 \\\n","  --index-url https://download.pytorch.org/whl/cu121\n","\n","# ╭────────────── Cell 2 : everything else we really need ─────────╮\n","!pip install --quiet --upgrade \\\n","  statsmodels==0.14.1 \\\n","  pmdarima==2.0.4 \\\n","  properscoring==0.1 \\\n","  optuna==3.6.1 \\\n","  tqdm==4.67.1 \\\n","  scikit-learn==1.5.0"],"metadata":{"collapsed":true,"id":"7NbsR7N3AE29","executionInfo":{"status":"ok","timestamp":1746645126483,"user_tz":300,"elapsed":5798,"user":{"displayName":"Preston Keskey","userId":"02221095565337972702"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Baseline Models: Global ARIMA (for reference) and LSTM to compare with BARNN"],"metadata":{"id":"sKsGE2NsGl2P"}},{"cell_type":"code","source":["# baseline_models.py ----------------------------------------------------------\n","# Baselines for \"Novel Applications of BARNN in Retail Sales\"\n","#   * Global ARIMA(1,1,1)\n","#   * Global LSTM (Optuna‑tuned, MC‑Dropout)\n","#   * Metrics: RMSLE / RMSE / MAE / CRPS / 90 % Coverage\n","# ----------------------------------------------------------------– 2025‑05‑07\n","\n","# 0. LIBRARIES & GLOBALS\n","import os, gc, math, random, warnings, time\n","import numpy as np, pandas as pd\n","from tqdm import tqdm\n","import statsmodels.api as sm\n","from statsmodels.tsa.arima.model import ARIMA\n","from properscoring import crps_ensemble\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","\n","import optuna, torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","torch.backends.cudnn.benchmark = True\n","\n","SEED         = 42\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","\n","DATA_DIR     = \"/content/drive/MyDrive/CSCI 5527 Project\"   # adjust if needed\n","TRAIN_CUTOFF = 31              # months 0‑30 train, 31‑33 test\n","SEQ_LEN      = 12\n","DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","N_MC         = 5              # MC‑Dropout samples\n","\n","# ---------------------------------------------------------------------------\n","print(\"[Data] loading …\")\n","sales = pd.read_csv(f\"{DATA_DIR}/sales_train.csv\")\n","\n","# monthly aggregation\n","monthly = (sales.groupby([\"date_block_num\", \"shop_id\", \"item_id\"], as_index=False)\n","                 .agg(item_cnt_month=(\"item_cnt_day\", \"sum\")))\n","\n","# dense 34‑month grid\n","grid = (monthly[[\"shop_id\", \"item_id\"]].drop_duplicates().assign(key=1)\n","          .merge(pd.DataFrame({\"date_block_num\": range(34), \"key\":1}), on=\"key\")\n","          .drop(\"key\", axis=1))\n","monthly = (grid.merge(monthly, how=\"left\")\n","                .fillna({\"item_cnt_month\": 0})\n","                .sort_values([\"shop_id\", \"item_id\", \"date_block_num\"]))\n","\n","# --- map raw IDs → contiguous indices for Embedding -------------------------\n","shop2idx = {sid: i for i, sid in enumerate(sorted(monthly.shop_id.unique()))}\n","item2idx = {iid: i for i, iid in enumerate(sorted(monthly.item_id.unique()))}\n","\n","monthly[\"shop_idx\"] = monthly.shop_id.map(shop2idx)\n","monthly[\"item_idx\"] = monthly.item_id.map(item2idx)\n","\n","N_SHOPS = len(shop2idx)\n","N_ITEMS = len(item2idx)\n","\n","# ---------------------------------------------------------------------------\n","# 1. METRICS\n","def _clip(x):               # ensure non‑negative for zero‑inflated RMSLE\n","    return np.maximum(0, np.asarray(x))\n","\n","def rmsle(y_t, y_p): return np.sqrt(np.mean((np.log1p(_clip(y_p)) -\n","                                             np.log1p(_clip(y_t)))**2))\n","def rmse(y_t, y_p): return np.sqrt(mean_squared_error(y_t, _clip(y_p)))\n","def mae (y_t, y_p): return mean_absolute_error(y_t, _clip(y_p))\n","def coverage(y_t, lo, hi):  return ((_clip(y_t) >= lo) & (_clip(y_t) <= hi)).mean()\n","\n","# ---------------------------------------------------------------------------\n","# 2. GLOBAL ARIMA(1,1,1)\n","print(\"\\n[Model] Global ARIMA(1,1,1) …\")\n","series = (monthly.groupby(\"date_block_num\")\n","                  .agg(total=(\"item_cnt_month\", \"sum\"))\n","                  .total)\n","train, test = series[:TRAIN_CUTOFF], series[TRAIN_CUTOFF:]\n","\n","arima = ARIMA(train, order=(1,1,1)).fit()\n","fc = arima.get_forecast(steps=len(test), alpha=0.10)\n","ar_mu = _clip(fc.predicted_mean.values)\n","ar_lo, ar_hi = fc.conf_int(alpha=0.10).T.values.clip(min=0)\n","\n","print(f\"ARIMA  | RMSLE {rmsle(test, ar_mu):.4f} | RMSE {rmse(test, ar_mu):.1f}\"\n","      f\" | MAE {mae(test, ar_mu):.1f} | Cov90 {coverage(test, ar_lo, ar_hi):.3f}\"\n","      f\" | CRPS {crps_ensemble(test.values, np.stack([ar_mu]*N_MC, 1)).mean():.3f}\")\n","\n","# ---------------------------------------------------------------------------\n","# 3. DATASET & MODEL for GLOBAL LSTM\n","class SalesDataset(Dataset):\n","    def __init__(self, frame, seq_len, mode):\n","        self.seq_len, self.mode = seq_len, mode\n","        # dict: (shop_idx, item_idx) -> 34‑length vector\n","        self.series = {k: g.sort_values(\"date_block_num\").item_cnt_month.values\n","                       for k, g in frame.groupby([\"shop_idx\", \"item_idx\"])}\n","        self.keys = list(self.series)\n","\n","    def __len__(self): return len(self.keys)\n","\n","    def __getitem__(self, idx):\n","        s, i = self.keys[idx]\n","        vec  = self.series[(s, i)]\n","        if self.mode == \"train\":\n","            t = np.random.randint(self.seq_len, TRAIN_CUTOFF)\n","            x, y = vec[t - self.seq_len: t], vec[t]\n","        else:                                # deterministic slice m30→m31\n","            x = vec[TRAIN_CUTOFF - self.seq_len: TRAIN_CUTOFF]\n","            y = vec[TRAIN_CUTOFF: TRAIN_CUTOFF + 3]          # not used here\n","        return (torch.tensor(s, dtype=torch.long),\n","                torch.tensor(i, dtype=torch.long),\n","                torch.tensor(x, dtype=torch.float32),\n","                torch.tensor(y, dtype=torch.float32))\n","\n","class GlobalLSTM(nn.Module):\n","    def __init__(self, n_shops, n_items, emb_dim, hidden, p_drop):\n","        super().__init__()\n","        self.shop_emb = nn.Embedding(n_shops, emb_dim)\n","        self.item_emb = nn.Embedding(n_items, emb_dim)\n","        self.lstm     = nn.LSTM(1 + 2*emb_dim, hidden, batch_first=True)\n","        self.drop     = nn.Dropout(p_drop)\n","        self.head     = nn.Linear(hidden, 1)\n","\n","    def forward(self, shop, item, x):        # x: (B, T)\n","        b, t = x.shape\n","        emb  = torch.cat([self.shop_emb(shop), self.item_emb(item)], -1)  # (B, 2*emb)\n","        emb  = emb.unsqueeze(1).repeat(1, t, 1)\n","        inp  = torch.cat([x.unsqueeze(-1), emb], -1)                      # (B, T, 1+2*emb)\n","        out, _ = self.lstm(inp)\n","        out   = self.drop(out[:, -1, :])\n","        return self.head(out).squeeze(1)\n","\n","train_ds = SalesDataset(monthly, SEQ_LEN, \"train\")\n","\n","# ---------------------------------------------------------------------------\n","# 4. OPTUNA TUNING (8 trials × 5 epochs)\n","def objective(trial):\n","    emb   = trial.suggest_int(\"emb\", 16, 48, step=16)\n","    hid   = trial.suggest_int(\"hid\", 32, 128, step=32)\n","    p_drop= trial.suggest_float(\"p_drop\", 0.1, 0.4)\n","    lr    = trial.suggest_loguniform(\"lr\", 1e-4, 3e-3)\n","\n","    model = GlobalLSTM(N_SHOPS, N_ITEMS, emb, hid, p_drop).to(DEVICE)\n","    opt   = torch.optim.Adam(model.parameters(), lr=lr)\n","    lossf = nn.MSELoss()\n","    loader= DataLoader(train_ds, batch_size=1024, shuffle=True,\n","                       num_workers=2, pin_memory=True)\n","\n","    model.train()\n","    for _ in range(5):        # 5 quick epochs\n","        for s,i,x,y in loader:\n","            s,i,x,y = s.to(DEVICE), i.to(DEVICE), x.to(DEVICE), y.to(DEVICE)\n","            opt.zero_grad()\n","            loss = lossf(model(s,i,x), y)\n","            loss.backward(); opt.step()\n","\n","    # simple validation: month 30→31\n","    vt, vp = [], []\n","    with torch.no_grad():\n","        for _ in range(8000):                      # subsample for speed\n","            s,i = random.choice(train_ds.keys)\n","            vec = train_ds.series[(s,i)]\n","            x   = torch.tensor(vec[TRAIN_CUTOFF-SEQ_LEN:TRAIN_CUTOFF],\n","                               dtype=torch.float32, device=DEVICE).unsqueeze(0)\n","            vt.append(vec[TRAIN_CUTOFF])\n","            vp.append(model(torch.tensor([s],device=DEVICE),\n","                            torch.tensor([i],device=DEVICE), x).cpu().item())\n","    return rmsle(vt, vp)\n","\n","print(\"\\n[Optuna] tuning global LSTM …\")\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=8, timeout=800, show_progress_bar=False)\n","best = study.best_params\n","print(\"Best hyper‑params:\", best)\n","\n","# ---------------------------------------------------------------------------\n","# 5. TRAIN BEST LSTM a few more epochs\n","model = GlobalLSTM(N_SHOPS, N_ITEMS,\n","                   best[\"emb\"], best[\"hid\"], best[\"p_drop\"]).to(DEVICE)\n","opt   = torch.optim.Adam(model.parameters(), lr=best[\"lr\"])\n","lossf = nn.MSELoss()\n","loader= DataLoader(train_ds, batch_size=1024, shuffle=True,\n","                   num_workers=2, pin_memory=True)\n","\n","print(\"\\n[LSTM] fine‑tuning …\")\n","for ep in range(1, 8):   # 7 epochs\n","    loss_sum, n = 0.0, 0\n","    model.train()\n","    for s,i,x,y in loader:\n","        s,i,x,y = s.to(DEVICE), i.to(DEVICE), x.to(DEVICE), y.to(DEVICE)\n","        opt.zero_grad()\n","        loss = lossf(model(s,i,x), y)\n","        loss.backward(); opt.step()\n","        loss_sum += loss.item() * y.size(0); n += y.size(0)\n","    print(f\"[{ep}/7] train MSE {loss_sum/n:.3f}\")\n","\n","# ---------------------------------------------------------------------------\n","# 6. ONE‑STEP FORECAST FOR METRICS (month 31)\n","def mc_pred(shop, item, vec):\n","    x = torch.tensor(vec[TRAIN_CUTOFF-SEQ_LEN: TRAIN_CUTOFF],\n","                     dtype=torch.float32, device=DEVICE).unsqueeze(0)\n","    s = torch.tensor([shop], device=DEVICE)\n","    i = torch.tensor([item], device=DEVICE)\n","    ys = []\n","    model.eval()\n","    with torch.no_grad():\n","        for _ in range(N_MC):\n","            ys.append(model(s,i,x).cpu().item())\n","    ys = np.array(ys)\n","    return ys.mean(), np.percentile(ys, [5, 95])\n","\n","true, mu, lo, hi = [], [], [], []\n","for (s,i), grp in monthly.groupby([\"shop_idx\", \"item_idx\"]):\n","    vec = grp.sort_values(\"date_block_num\").item_cnt_month.values\n","    m, (l,u) = mc_pred(s,i,vec)\n","    true.append(vec[TRAIN_CUTOFF]); mu.append(_clip(m)); lo.append(_clip(l)); hi.append(_clip(u))\n","\n","print(f\"\\nLSTM   | RMSLE {rmsle(true, mu):.4f} | RMSE {rmse(true, mu):.1f}\"\n","      f\" | MAE {mae(true, mu):.1f} | Cov90 {coverage(true, lo, hi):.3f}\"\n","      f\" | CRPS {crps_ensemble(np.array(true), np.stack([mu]*N_MC,1)).mean():.3f}\")\n","\n","# ---------------------------------------------------------------------------\n","# 7. SUMMARY TABLE\n","print(\"\\n------------- Baseline Summary -------------\")\n","print(\"Model | RMSLE |  RMSE |  MAE | Cov90 | CRPS\")\n","print(\"------|-------|-------|------|-------|------\")\n","print(f\"ARIMA | {rmsle(test, ar_mu):.4f} | {rmse(test, ar_mu):7.1f} |\"\n","      f\" {mae(test, ar_mu):6.1f} | {coverage(test, ar_lo, ar_hi):.3f} |\"\n","      f\" {crps_ensemble(test.values, np.stack([ar_mu]*N_MC,1)).mean():.3f}\")\n","print(f\"LSTM  | {rmsle(true, mu):.4f} | {rmse(true, mu):7.1f} |\"\n","      f\" {mae(true, mu):6.1f} | {coverage(true, lo, hi):.3f} |\"\n","      f\" {crps_ensemble(np.array(true), np.stack([mu]*N_MC,1)).mean():.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3tCfPq4mJauR","executionInfo":{"status":"ok","timestamp":1746650278247,"user_tz":300,"elapsed":5141510,"user":{"displayName":"Preston Keskey","userId":"02221095565337972702"}},"outputId":"e9684469-ff26-4a2e-911c-11a5416deed0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[Data] loading …\n","\n","[Model] Global ARIMA(1,1,1) …\n","ARIMA  | RMSLE 0.0255 | RMSE 1787.3 | MAE 1662.4 | Cov90 1.000 | CRPS 1662.384\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-05-07 19:14:25,511] A new study created in memory with name: no-name-d47c3598-6e6f-4cbe-b39b-7dfe0d5c84be\n"]},{"output_type":"stream","name":"stdout","text":["\n","[Optuna] tuning global LSTM …\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-05-07 19:15:19,555] Trial 0 finished with value: 0.25146524357596356 and parameters: {'emb': 48, 'hid': 96, 'p_drop': 0.10292988725822107, 'lr': 0.002708020061719506}. Best is trial 0 with value: 0.25146524357596356.\n","[I 2025-05-07 19:16:12,450] Trial 1 finished with value: 0.2600826966679851 and parameters: {'emb': 48, 'hid': 96, 'p_drop': 0.14866514729890173, 'lr': 0.00018994160650370794}. Best is trial 0 with value: 0.25146524357596356.\n","[I 2025-05-07 19:17:05,162] Trial 2 finished with value: 0.26114867126268543 and parameters: {'emb': 32, 'hid': 64, 'p_drop': 0.3762732506199805, 'lr': 0.0009838766274203484}. Best is trial 0 with value: 0.25146524357596356.\n","[I 2025-05-07 19:17:57,928] Trial 3 finished with value: 0.2518365889296712 and parameters: {'emb': 32, 'hid': 64, 'p_drop': 0.13586092073548445, 'lr': 0.002442198479596263}. Best is trial 0 with value: 0.25146524357596356.\n","[I 2025-05-07 19:18:50,443] Trial 4 finished with value: 0.2358203583449319 and parameters: {'emb': 16, 'hid': 96, 'p_drop': 0.1824822277445423, 'lr': 0.0012243823179153152}. Best is trial 4 with value: 0.2358203583449319.\n","[I 2025-05-07 19:19:43,569] Trial 5 finished with value: 0.25783001213291223 and parameters: {'emb': 32, 'hid': 96, 'p_drop': 0.3506276115386311, 'lr': 0.000632412823142246}. Best is trial 4 with value: 0.2358203583449319.\n","[I 2025-05-07 19:20:36,225] Trial 6 finished with value: 0.25922076750663464 and parameters: {'emb': 16, 'hid': 128, 'p_drop': 0.2347205036708416, 'lr': 0.0012461253286884248}. Best is trial 4 with value: 0.2358203583449319.\n","[I 2025-05-07 19:21:28,711] Trial 7 finished with value: 0.2645445570868655 and parameters: {'emb': 48, 'hid': 96, 'p_drop': 0.13805570404327533, 'lr': 0.0008486874333153081}. Best is trial 4 with value: 0.2358203583449319.\n"]},{"output_type":"stream","name":"stdout","text":["Best hyper‑params: {'emb': 16, 'hid': 96, 'p_drop': 0.1824822277445423, 'lr': 0.0012243823179153152}\n","\n","[LSTM] fine‑tuning …\n","[1/7] train MSE 12.139\n","[2/7] train MSE 6.768\n","[3/7] train MSE 6.688\n","[4/7] train MSE 7.157\n","[5/7] train MSE 7.690\n","[6/7] train MSE 4.762\n","[7/7] train MSE 8.294\n","\n","LSTM   | RMSLE 0.2457 | RMSE 1.3 | MAE 0.2 | Cov90 0.071 | CRPS 0.230\n","\n","------------- Baseline Summary -------------\n","Model | RMSLE |  RMSE |  MAE | Cov90 | CRPS\n","------|-------|-------|------|-------|------\n","ARIMA | 0.0255 |  1787.3 | 1662.4 | 1.000 | 1662.384\n","LSTM  | 0.2457 |     1.3 |    0.2 | 0.071 | 0.230\n"]}]},{"cell_type":"markdown","source":["# Baseline Models: Global ARIMA; Per Series Global ARIMA and LSTM to compare with BARNN"],"metadata":{"id":"78f8lFtSf0HR"}},{"cell_type":"code","source":["# baseline_models.py ----------------------------------------------------------\n","# Baselines for \"Novel Applications of BARNN in Retail Sales\"\n","#   • ARIMA(1,1,1) on the AGGREGATE total\n","#   • ARIMA(1,1,1) on EVERY shop‑item series  (new!)\n","#   • Global LSTM (Optuna‑tuned, MC‑Dropout)\n","#   • Metrics: RMSLE / RMSE / MAE / CRPS / 90 % Coverage\n","# ----------------------------------------------------------------– 2025‑05‑07\n","\n","# 0. LIBRARIES & GLOBALS\n","import os, random, warnings, time, math, gc\n","import numpy as np, pandas as pd\n","from tqdm import tqdm\n","import statsmodels.api as sm\n","from statsmodels.tsa.arima.model import ARIMA\n","from properscoring import crps_ensemble\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","\n","import optuna, torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from joblib import Parallel, delayed\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","torch.backends.cudnn.benchmark = True\n","\n","SEED = 42\n","random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n","\n","DATA_DIR     = \"/content/drive/MyDrive/CSCI 5527 Project\"\n","TRAIN_CUTOFF = 31               # months 0‑30 train; 31‑33 test\n","SEQ_LEN      = 12\n","DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","N_MC         = 5                # MC‑Dropout draws\n","\n","# ────────────────────────────────────────────────────────────────────────────\n","print(\"[Data] loading …\")\n","sales  = pd.read_csv(f\"{DATA_DIR}/sales_train.csv\")\n","\n","monthly = (sales.groupby([\"date_block_num\",\"shop_id\",\"item_id\"], as_index=False)\n","                 .agg(item_cnt_month=(\"item_cnt_day\",\"sum\")))\n","\n","grid = (monthly[[\"shop_id\",\"item_id\"]].drop_duplicates().assign(key=1)\n","          .merge(pd.DataFrame({\"date_block_num\": range(34), \"key\":1}), on=\"key\")\n","          .drop(\"key\", axis=1))\n","monthly = (grid.merge(monthly, how=\"left\")\n","                .fillna({\"item_cnt_month\":0})\n","                .sort_values([\"shop_id\",\"item_id\",\"date_block_num\"]))\n","\n","shop2idx = {sid:i for i,sid in enumerate(sorted(monthly.shop_id.unique()))}\n","item2idx = {iid:i for i,iid in enumerate(sorted(monthly.item_id.unique()))}\n","monthly[\"shop_idx\"] = monthly.shop_id.map(shop2idx)\n","monthly[\"item_idx\"] = monthly.item_id.map(item2idx)\n","\n","N_SHOPS, N_ITEMS = len(shop2idx), len(item2idx)\n","\n","# ─── metric helpers ─────────────────────────────────────────────────────────\n","_clip = lambda x: np.maximum(0, np.asarray(x))\n","def rmsle(y_t,y_p): return np.sqrt(np.mean((np.log1p(_clip(y_p))-\n","                                            np.log1p(_clip(y_t)))**2))\n","def rmse(y_t,y_p):  return np.sqrt(mean_squared_error(y_t,_clip(y_p)))\n","def mae (y_t,y_p):  return mean_absolute_error(y_t,_clip(y_p))\n","def coverage(y_t,lo,hi): return ((_clip(y_t)>=lo)&(_clip(y_t)<=hi)).mean()\n","\n","# ---------------------------------------------------------------------------\n","# 1. PER‑SERIES  GLOBAL  ARIMA(1,1,1)  (new section)\n","# ---------------------------------------------------------------------------\n","print(\"\\n[Per‑series ARIMA] fitting ≈420 k series …\")\n","\n","def forecast_one(vec):\n","    try:\n","        m  = ARIMA(vec[:TRAIN_CUTOFF], order=(1,1,1)).fit(method=\"statespace\",\n","                                                          disp=0)\n","        fc = m.forecast(1)[0]\n","    except Exception:                    # rare convergence fail\n","        fc = vec[TRAIN_CUTOFF-1]         # naive fallback\n","    return max(fc,0.0)\n","\n","series_vecs = (monthly.groupby([\"shop_idx\",\"item_idx\"])\n","                        .apply(lambda g: g.item_cnt_month.values)\n","                        .tolist())\n","\n","ps_preds = Parallel(n_jobs=-1, verbose=0)(\n","              delayed(forecast_one)(v) for v in series_vecs)\n","\n","ps_true = monthly.loc[monthly.date_block_num==TRAIN_CUTOFF,\n","                      \"item_cnt_month\"].values\n","\n","ps_lo = _clip(ps_preds - 1.64*np.sqrt(ps_preds))\n","ps_hi = _clip(ps_preds + 1.64*np.sqrt(ps_preds))\n","\n","print(f\"ARIMA‑series | RMSLE {rmsle(ps_true, ps_preds):.4f}\"\n","      f\" | RMSE {rmse(ps_true, ps_preds):.2f}\"\n","      f\" | MAE {mae(ps_true, ps_preds):.2f}\"\n","      f\" | Cov90 {coverage(ps_true, ps_lo, ps_hi):.3f}\"\n","      f\" | CRPS {crps_ensemble(ps_true, np.stack([ps_preds]*N_MC,1)).mean():.3f}\")\n","\n","# ---------------------------------------------------------------------------\n","# 2. GLOBAL  (aggregate‑total)  ARIMA(1,1,1)\n","# ---------------------------------------------------------------------------\n","print(\"\\n[Model] Global ARIMA(1,1,1) on TOTAL sales …\")\n","total_series = (monthly.groupby(\"date_block_num\")\n","                        .agg(total=(\"item_cnt_month\",\"sum\"))\n","                        .total)\n","train_tot, test_tot = total_series[:TRAIN_CUTOFF], total_series[TRAIN_CUTOFF:]\n","\n","arima_tot = ARIMA(train_tot, order=(1,1,1)).fit()\n","fc_tot  = arima_tot.get_forecast(steps=len(test_tot), alpha=0.10)\n","tot_mu  = _clip(fc_tot.predicted_mean.values)\n","tot_lo, tot_hi = fc_tot.conf_int(alpha=0.10).T.values.clip(min=0)\n","\n","print(f\"ARIMA‑total  | RMSLE {rmsle(test_tot, tot_mu):.4f}\"\n","      f\" | RMSE {rmse(test_tot, tot_mu):.1f}\"\n","      f\" | MAE {mae (test_tot, tot_mu):.1f}\"\n","      f\" | Cov90 {coverage(test_tot, tot_lo, tot_hi):.3f}\"\n","      f\" | CRPS {crps_ensemble(test_tot.values, np.stack([tot_mu]*N_MC,1)).mean():.3f}\")\n","\n","# ---------------------------------------------------------------------------\n","# 3. GLOBAL LSTM with shop/item embeddings  (unchanged)\n","# ---------------------------------------------------------------------------\n","class SalesDataset(Dataset):\n","    def __init__(self, frame, seq_len, mode):\n","        self.seq_len, self.mode = seq_len, mode\n","        self.series = {k: g.sort_values(\"date_block_num\").item_cnt_month.values\n","                       for k,g in frame.groupby([\"shop_idx\",\"item_idx\"])}\n","        self.keys = list(self.series)\n","    def __len__(self): return len(self.keys)\n","    def __getitem__(self, idx):\n","        s,i = self.keys[idx]; vec=self.series[(s,i)]\n","        if self.mode==\"train\":\n","            t = np.random.randint(self.seq_len, TRAIN_CUTOFF)\n","            x,y = vec[t-self.seq_len:t], vec[t]\n","        else:\n","            x = vec[TRAIN_CUTOFF-self.seq_len:TRAIN_CUTOFF]\n","            y = vec[TRAIN_CUTOFF:TRAIN_CUTOFF+3]\n","        return (torch.tensor(s), torch.tensor(i),\n","                torch.tensor(x,dtype=torch.float32),\n","                torch.tensor(y,dtype=torch.float32))\n","\n","class GlobalLSTM(nn.Module):\n","    def __init__(self,n_shops,n_items,emb,hid,p_drop):\n","        super().__init__()\n","        self.shop_emb = nn.Embedding(n_shops,emb)\n","        self.item_emb = nn.Embedding(n_items,emb)\n","        self.lstm     = nn.LSTM(1+2*emb,hid,batch_first=True)\n","        self.drop     = nn.Dropout(p_drop)\n","        self.head     = nn.Linear(hid,1)\n","    def forward(self,shop,item,x):\n","        b,t = x.shape\n","        e = torch.cat([self.shop_emb(shop), self.item_emb(item)],-1)\n","        e = e.unsqueeze(1).repeat(1,t,1)\n","        z = torch.cat([x.unsqueeze(-1), e],-1)\n","        h,_ = self.lstm(z)\n","        h = self.drop(h[:,-1,:])\n","        return self.head(h).squeeze(1)\n","\n","train_ds = SalesDataset(monthly,SEQ_LEN,\"train\")\n","\n","# --- Optuna hyper‑search -----------------------------------------------------\n","def objective(trial):\n","    emb   = trial.suggest_int (\"emb\",16,48,step=16)\n","    hid   = trial.suggest_int (\"hid\",32,128,step=32)\n","    p_drop= trial.suggest_float(\"p_drop\",0.1,0.4)\n","    lr    = trial.suggest_loguniform(\"lr\",1e-4,3e-3)\n","    mdl   = GlobalLSTM(N_SHOPS,N_ITEMS,emb,hid,p_drop).to(DEVICE)\n","    opt   = torch.optim.Adam(mdl.parameters(),lr=lr)\n","    lossf = nn.MSELoss()\n","    loader= DataLoader(train_ds,batch_size=1024,shuffle=True,\n","                       num_workers=2,pin_memory=True)\n","    mdl.train()\n","    for _ in range(5):\n","        for s,i,x,y in loader:\n","            s,i,x,y = s.to(DEVICE),i.to(DEVICE),x.to(DEVICE),y.to(DEVICE)\n","            opt.zero_grad(); loss=lossf(mdl(s,i,x),y); loss.backward(); opt.step()\n","    # quick val m30→m31\n","    vt,vp=[],[]\n","    with torch.no_grad():\n","        for _ in range(8000):\n","            s,i = random.choice(train_ds.keys)\n","            vec = train_ds.series[(s,i)]\n","            x = torch.tensor(vec[TRAIN_CUTOFF-SEQ_LEN:TRAIN_CUTOFF],\n","                             dtype=torch.float32,device=DEVICE).unsqueeze(0)\n","            vt.append(vec[TRAIN_CUTOFF])\n","            vp.append(mdl(torch.tensor([s],device=DEVICE),\n","                          torch.tensor([i],device=DEVICE),x).cpu().item())\n","    return rmsle(vt,vp)\n","\n","print(\"\\n[Optuna] tuning global LSTM …\")\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective,n_trials=8,timeout=800,show_progress_bar=False)\n","best = study.best_params\n","print(\"Best hyper‑params:\",best)\n","\n","# --- train best config -------------------------------------------------------\n","lstm = GlobalLSTM(N_SHOPS,N_ITEMS,best[\"emb\"],best[\"hid\"],best[\"p_drop\"]).to(DEVICE)\n","opt  = torch.optim.Adam(lstm.parameters(),lr=best[\"lr\"])\n","lossf= nn.MSELoss()\n","loader=DataLoader(train_ds,batch_size=1024,shuffle=True,num_workers=2,pin_memory=True)\n","\n","print(\"\\n[LSTM] fine‑tuning …\")\n","for ep in range(1,8):\n","    loss_sum,n=0,0\n","    lstm.train()\n","    for s,i,x,y in loader:\n","        s,i,x,y=s.to(DEVICE),i.to(DEVICE),x.to(DEVICE),y.to(DEVICE)\n","        opt.zero_grad(); loss=lossf(lstm(s,i,x),y); loss.backward(); opt.step()\n","        loss_sum+=loss.item()*y.size(0); n+=y.size(0)\n","    print(f\"[{ep}/7] train MSE {loss_sum/n:.3f}\")\n","\n","# --- batched MC‑Dropout inference month‑31 ----------------------------------\n","print(\"\\n[LSTM] MC‑Dropout inference …\")\n","series_keys = list(monthly.groupby([\"shop_idx\",\"item_idx\"]).groups.keys())\n","X_all = np.stack([monthly.loc[(monthly.shop_idx==s)&(monthly.item_idx==i),\n","                              \"item_cnt_month\"].values\n","                  [TRAIN_CUTOFF-SEQ_LEN:TRAIN_CUTOFF] for (s,i) in series_keys])\n","X_all = torch.tensor(X_all,dtype=torch.float32,device=DEVICE)\n","S_all = torch.tensor([k[0] for k in series_keys],dtype=torch.long,device=DEVICE)\n","I_all = torch.tensor([k[1] for k in series_keys],dtype=torch.long,device=DEVICE)\n","\n","lstm.eval(); preds=[]\n","with torch.no_grad():\n","    for _ in range(N_MC):\n","        preds.append(lstm(S_all,I_all,X_all).cpu().numpy())\n","preds = np.stack(preds)          # (N_MC, N_series)\n","\n","mu  = _clip(preds.mean(0))\n","lo  = _clip(np.percentile(preds,5,0))\n","hi  = _clip(np.percentile(preds,95,0))\n","lstm_true = monthly.loc[monthly.date_block_num==TRAIN_CUTOFF,\n","                        \"item_cnt_month\"].values\n","\n","print(f\"LSTM           | RMSLE {rmsle(lstm_true,mu):.4f}\"\n","      f\" | RMSE {rmse(lstm_true,mu):.2f}\"\n","      f\" | MAE {mae(lstm_true,mu):.2f}\"\n","      f\" | Cov90 {coverage(lstm_true,lo,hi):.3f}\"\n","      f\" | CRPS {crps_ensemble(lstm_true,np.stack([mu]*N_MC,1)).mean():.3f}\")\n","\n","# ---------------------------------------------------------------------------\n","# 4. SUMMARY TABLE\n","print(\"\\n------------- Baseline Summary -------------\")\n","print(\"Model              | RMSLE |  RMSE |  MAE | Cov90 |  CRPS\")\n","print(\"-------------------|-------|-------|------|-------|-------\")\n","print(f\"ARIMA (total)      | {rmsle(test_tot, tot_mu):.4f} |\"\n","      f\" {rmse(test_tot, tot_mu):7.1f} | {mae(test_tot, tot_mu):6.1f} |\"\n","      f\" {coverage(test_tot, tot_lo, tot_hi):.3f} |\"\n","      f\" {crps_ensemble(test_tot.values, np.stack([tot_mu]*N_MC,1)).mean():.3f}\")\n","print(f\"ARIMA (per‑series) | {rmsle(ps_true, ps_preds):.4f} |\"\n","      f\" {rmse(ps_true, ps_preds):7.1f} | {mae(ps_true, ps_preds):6.1f} |\"\n","      f\" {coverage(ps_true, ps_lo, ps_hi):.3f} |\"\n","      f\" {crps_ensemble(ps_true, np.stack([ps_preds]*N_MC,1)).mean():.3f}\")\n","print(f\"LSTM              | {rmsle(lstm_true, mu):.4f} |\"\n","      f\" {rmse(lstm_true, mu):7.1f} | {mae(lstm_true, mu):6.1f} |\"\n","      f\" {coverage(lstm_true, lo, hi):.3f} |\"\n","      f\" {crps_ensemble(lstm_true, np.stack([mu]*N_MC,1)).mean():.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhg3-42lhNZi","executionInfo":{"status":"ok","timestamp":1746659669476,"user_tz":300,"elapsed":8245538,"user":{"displayName":"Preston Keskey","userId":"02221095565337972702"}},"outputId":"090c2a89-1abc-48a3-8ab0-959e4f584044"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[Data] loading …\n","\n","[Per‑series ARIMA] fitting ≈420 k series …\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-4a719b4f074c>:78: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda g: g.item_cnt_month.values)\n"]},{"output_type":"stream","name":"stdout","text":["ARIMA‑series | RMSLE 0.2743 | RMSE 0.95 | MAE 0.17 | Cov90 0.947 | CRPS 0.175\n","\n","[Model] Global ARIMA(1,1,1) on TOTAL sales …\n","ARIMA‑total  | RMSLE 0.0255 | RMSE 1787.3 | MAE 1662.4 | Cov90 1.000 | CRPS 1662.384\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-05-07 21:00:37,272] A new study created in memory with name: no-name-f8e0f615-d30d-42c3-a18f-c1bbbc8c99cc\n"]},{"output_type":"stream","name":"stdout","text":["\n","[Optuna] tuning global LSTM …\n"]},{"output_type":"stream","name":"stderr","text":["[I 2025-05-07 21:01:32,677] Trial 0 finished with value: 0.26358592260941854 and parameters: {'emb': 32, 'hid': 32, 'p_drop': 0.36848230474234633, 'lr': 0.00022518169953072038}. Best is trial 0 with value: 0.26358592260941854.\n","[I 2025-05-07 21:02:28,382] Trial 1 finished with value: 0.26703782405911347 and parameters: {'emb': 32, 'hid': 32, 'p_drop': 0.3405953770231652, 'lr': 0.00012697929663038347}. Best is trial 0 with value: 0.26358592260941854.\n","[I 2025-05-07 21:03:24,791] Trial 2 finished with value: 0.2545548770101176 and parameters: {'emb': 32, 'hid': 64, 'p_drop': 0.13487245436940312, 'lr': 0.0001336788568795171}. Best is trial 2 with value: 0.2545548770101176.\n","[I 2025-05-07 21:04:21,017] Trial 3 finished with value: 0.26132730764400175 and parameters: {'emb': 32, 'hid': 128, 'p_drop': 0.19449674194885913, 'lr': 0.00024538376249076395}. Best is trial 2 with value: 0.2545548770101176.\n","[I 2025-05-07 21:05:16,576] Trial 4 finished with value: 0.2529667528139925 and parameters: {'emb': 32, 'hid': 64, 'p_drop': 0.14354055360096712, 'lr': 0.0009971250309766461}. Best is trial 4 with value: 0.2529667528139925.\n","[I 2025-05-07 21:06:12,314] Trial 5 finished with value: 0.24656144364140875 and parameters: {'emb': 16, 'hid': 64, 'p_drop': 0.35862704860509154, 'lr': 0.0022478322078263806}. Best is trial 5 with value: 0.24656144364140875.\n","[I 2025-05-07 21:07:08,053] Trial 6 finished with value: 0.2615712771776539 and parameters: {'emb': 32, 'hid': 64, 'p_drop': 0.1765260236716384, 'lr': 0.0007931600746808351}. Best is trial 5 with value: 0.24656144364140875.\n","[I 2025-05-07 21:08:03,533] Trial 7 finished with value: 0.2523053820595675 and parameters: {'emb': 16, 'hid': 96, 'p_drop': 0.3366570454797119, 'lr': 0.002619600891102374}. Best is trial 5 with value: 0.24656144364140875.\n"]},{"output_type":"stream","name":"stdout","text":["Best hyper‑params: {'emb': 16, 'hid': 64, 'p_drop': 0.35862704860509154, 'lr': 0.0022478322078263806}\n","\n","[LSTM] fine‑tuning …\n","[1/7] train MSE 7.878\n","[2/7] train MSE 9.195\n","[3/7] train MSE 7.060\n","[4/7] train MSE 11.007\n","[5/7] train MSE 8.140\n","[6/7] train MSE 9.668\n","[7/7] train MSE 4.886\n","\n","[LSTM] MC‑Dropout inference …\n","LSTM           | RMSLE 0.2542 | RMSE 1.26 | MAE 0.25 | Cov90 0.027 | CRPS 0.255\n","\n","------------- Baseline Summary -------------\n","Model              | RMSLE |  RMSE |  MAE | Cov90 |  CRPS\n","-------------------|-------|-------|------|-------|-------\n","ARIMA (total)      | 0.0255 |  1787.3 | 1662.4 | 1.000 | 1662.384\n","ARIMA (per‑series) | 0.2743 |     0.9 |    0.2 | 0.947 | 0.175\n","LSTM              | 0.2542 |     1.3 |    0.3 | 0.027 | 0.255\n"]}]}]}